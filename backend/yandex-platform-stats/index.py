'''
Business: –ü–æ–ª—É—á–µ–Ω–∏–µ –∏ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–ª–æ—â–∞–¥–æ–∫ –∏–∑ –Ø–Ω–¥–µ–∫—Å.–î–∏—Ä–µ–∫—Ç + Admin Stats
Args: event —Å httpMethod, body (project_id, campaign_ids, date_from, date_to, goal_id, force_refresh) –∏–ª–∏ queryStringParameters.stats
Returns: JSON —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –ø–ª–æ—â–∞–¥–æ–∫ –∏–∑ –∫–µ—à–∞ –∏–ª–∏ API, –ª–∏–±–æ admin stats
'''

import json
import os
import psycopg2
from psycopg2.extras import RealDictCursor
from datetime import datetime, date
import requests

def handle_admin_stats(stats_type: str) -> dict:
    """Admin —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∑–∞–¥–∞—á–∞–º"""
    try:
        dsn = os.environ.get('DATABASE_URL')
        conn = psycopg2.connect(dsn)
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        
        if stats_type == 'tasks':
            cursor.execute('''
                SELECT 
                    t.id, 
                    t.description, 
                    t.enabled, 
                    p.name as project_name,
                    COUNT(DISTINCT bq.id) as total_placements,
                    COUNT(DISTINCT CASE WHEN bq.status = 'blocked' THEN bq.id END) as blocked_placements,
                    COUNT(DISTINCT CASE WHEN bq.status = 'pending' THEN bq.id END) as pending_placements,
                    COALESCE(SUM(bq.cost), 0) as total_cost
                FROM rsya_tasks t
                JOIN rsya_projects p ON t.project_id = p.id
                LEFT JOIN block_queue bq ON bq.task_id = t.id
                GROUP BY t.id, t.description, t.enabled, p.name
                ORDER BY t.id DESC
            ''')
            tasks = cursor.fetchall()
            
            cursor.close()
            conn.close()
            
            return {
                'statusCode': 200,
                'headers': {
                    'Access-Control-Allow-Origin': '*',
                    'Content-Type': 'application/json'
                },
                'body': json.dumps({'tasks': tasks}, default=str)
            }
        
        else:  # overview
            cursor.execute('''
                SELECT execution_type, COUNT(*) as executions,
                       SUM(placements_found) as total_found,
                       SUM(placements_blocked) as total_blocked
                FROM rsya_cleaning_execution_logs
                GROUP BY execution_type
            ''')
            execution_stats = cursor.fetchall()
            
            cursor.execute('''
                SELECT status, COUNT(*) as count
                FROM block_queue
                GROUP BY status
            ''')
            queue_stats = cursor.fetchall()
            
            cursor.execute('''
                SELECT 
                    el.id, el.execution_type, el.started_at,
                    el.placements_found, el.placements_matched,
                    el.placements_sent_to_queue, el.placements_blocked,
                    p.name as project_name, t.description as task_description
                FROM rsya_cleaning_execution_logs el
                LEFT JOIN rsya_projects p ON el.project_id = p.id
                LEFT JOIN rsya_tasks t ON el.task_id = t.id
                ORDER BY el.started_at DESC
                LIMIT 10
            ''')
            recent_executions = cursor.fetchall()
            
            cursor.close()
            conn.close()
            
            return {
                'statusCode': 200,
                'headers': {
                    'Access-Control-Allow-Origin': '*',
                    'Content-Type': 'application/json'
                },
                'body': json.dumps({
                    'execution_stats': execution_stats,
                    'queue_stats': queue_stats,
                    'recent_executions': recent_executions
                }, default=str)
            }
    
    except Exception as e:
        print(f'‚ùå Admin Stats Error: {str(e)}')
        return {
            'statusCode': 500,
            'headers': {'Access-Control-Allow-Origin': '*'},
            'body': json.dumps({'error': str(e)})
        }

def handler(event: dict, context) -> dict:
    method = event.get('httpMethod', 'GET')
    query_params = event.get('queryStringParameters', {}) or {}
    
    # ADMIN STATS —ç–Ω–¥–ø–æ–∏–Ω—Ç
    if method == 'GET' and query_params.get('stats'):
        return handle_admin_stats(query_params.get('stats'))
    
    # DEBUG —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ TSV
    if method == 'GET' and query_params.get('debug') == 'tsv':
        # –î–æ—Å—Ç–∞–µ–º –∏–∑ –ë–î –ø–æ—Å–ª–µ–¥–Ω—é—é —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é TSV —Å—Ç—Ä–æ–∫—É
        body_str = event.get('body') or '{}'
        return {
            'statusCode': 200,
            'headers': {'Access-Control-Allow-Origin': '*', 'Content-Type': 'text/plain'},
            'body': 'Debug endpoint - check logs for TSV data'
        }
    
    if method == 'OPTIONS':
        return {
            'statusCode': 200,
            'headers': {
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
                'Access-Control-Allow-Headers': 'Content-Type, X-User-Id, X-Auth-Token',
                'Access-Control-Max-Age': '86400'
            },
            'body': ''
        }
    
    if method != 'POST':
        return {
            'statusCode': 405,
            'headers': {'Access-Control-Allow-Origin': '*', 'Content-Type': 'application/json'},
            'body': json.dumps({'error': 'Method not allowed'})
        }
    
    try:
        body_str = event.get('body') or '{}'
        body = json.loads(body_str) if body_str else {}
        project_id = body.get('project_id')
        campaign_ids = body.get('campaign_ids', [])
        date_from = body.get('date_from')
        date_to = body.get('date_to')
        goal_id = body.get('goal_id', '')
        force_refresh = body.get('force_refresh', False)
        
        auth_token = event.get('headers', {}).get('X-Auth-Token', '')
        
        if not project_id or not campaign_ids or not date_from or not date_to:
            return {
                'statusCode': 400,
                'headers': {'Access-Control-Allow-Origin': '*', 'Content-Type': 'application/json'},
                'body': json.dumps({'error': 'Missing required fields'})
            }
        
        dsn = os.environ.get('DATABASE_URL')
        conn = psycopg2.connect(dsn)
        
        # –ï—Å–ª–∏ –Ω–µ force_refresh, –ø—ã—Ç–∞–µ–º—Å—è –≤–∑—è—Ç—å –∏–∑ –∫–µ—à–∞
        if not force_refresh:
            cached_count = check_cache_exists(conn, project_id, campaign_ids, date_from, date_to, goal_id)
            if cached_count > 0:
                conn.close()
                print(f'‚úÖ Cache exists: {cached_count} platforms. Please use direct DB query.')
                return {
                    'statusCode': 200,
                    'headers': {'Access-Control-Allow-Origin': '*', 'Content-Type': 'application/json'},
                    'body': json.dumps({
                        'from_cache': True, 
                        'platforms_count': cached_count,
                        'message': 'Data is cached. Use direct DB query to fetch platforms.'
                    })
                }
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–≤–µ–∂—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑ API
        fresh_stats = fetch_stats_from_api(auth_token, campaign_ids, date_from, date_to, goal_id)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
        save_stats_to_cache(conn, project_id, fresh_stats, date_from, date_to, goal_id)
        
        conn.close()
        
        print(f'‚úÖ Saved {len(fresh_stats)} platforms to cache')
        
        # –ù–ï –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 7496 –ø–ª–æ—â–∞–¥–æ–∫ –≤ –æ—Ç–≤–µ—Ç–µ - —ç—Ç–æ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π JSON!
        # –§—Ä–æ–Ω—Ç–µ–Ω–¥ —Å–¥–µ–ª–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å from_cache=False –∏ –ø–æ–ª—É—á–∏—Ç –¥–∞–Ω–Ω—ã–µ
        return {
            'statusCode': 200,
            'headers': {'Access-Control-Allow-Origin': '*', 'Content-Type': 'application/json'},
            'body': json.dumps({
                'success': True,
                'platforms_count': len(fresh_stats),
                'message': 'Data cached successfully. Please reload to fetch from cache.'
            })
        }
        
    except Exception as e:
        print(f'Error: {str(e)}')
        return {
            'statusCode': 500,
            'headers': {'Access-Control-Allow-Origin': '*', 'Content-Type': 'application/json'},
            'body': json.dumps({'error': str(e)})
        }

def check_cache_exists(conn, project_id: int, campaign_ids: list, date_from: str, date_to: str, goal_id: str):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∫–µ—à–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π"""
    cur = conn.cursor()
    
    campaign_ids_str = ','.join([f"'{cid}'" for cid in campaign_ids])
    
    query = f"""
        SELECT COUNT(*) as count
        FROM t_p97630513_yandex_cleaning_serv.rsya_platform_stats
        WHERE project_id = {project_id}
          AND campaign_id IN ({campaign_ids_str})
          AND date_from = '{date_from}'
          AND date_to = '{date_to}'
          AND COALESCE(goal_id, '') = '{goal_id}'
    """
    
    cur.execute(query)
    result = cur.fetchone()
    cur.close()
    
    return result[0] if result else 0

def get_cached_stats(conn, project_id: int, campaign_ids: list, date_from: str, date_to: str, goal_id: str):
    cur = conn.cursor(cursor_factory=RealDictCursor)
    
    campaign_ids_str = ','.join([f"'{cid}'" for cid in campaign_ids])
    
    query = f"""
        SELECT campaign_id, campaign_name, url, impressions, clicks, cost, 
               conversions, ctr, cpc, cpa, is_blocked, 
               COALESCE(status, CASE WHEN is_blocked THEN 'blocked' ELSE 'active' END) as status
        FROM t_p97630513_yandex_cleaning_serv.rsya_platform_stats
        WHERE project_id = {project_id}
          AND campaign_id IN ({campaign_ids_str})
          AND date_from = '{date_from}'
          AND date_to = '{date_to}'
          AND COALESCE(goal_id, '') = '{goal_id}'
    """
    
    cur.execute(query)
    rows = cur.fetchall()
    cur.close()
    
    if not rows:
        return None
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º Decimal –≤ float –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    result = []
    for row in rows:
        row_dict = dict(row)
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ Decimal –ø–æ–ª—è –≤ float
        for key in ['cost', 'ctr', 'cpc', 'cpa']:
            if key in row_dict and row_dict[key] is not None:
                row_dict[key] = float(row_dict[key])
        result.append(row_dict)
    
    return result

def save_stats_to_cache(conn, project_id: int, stats: list, date_from: str, date_to: str, goal_id: str):
    print(f'üíæ Saving {len(stats)} platforms to cache...')
    cur = conn.cursor()
    
    # –°–Ω–∞—á–∞–ª–∞ —É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ –∏ –ø–µ—Ä–∏–æ–¥–∞
    delete_query = f"""
        DELETE FROM t_p97630513_yandex_cleaning_serv.rsya_platform_stats
        WHERE project_id = {project_id}
          AND date_from = '{date_from}'
          AND date_to = '{date_to}'
          AND COALESCE(goal_id, '') = '{goal_id}'
    """
    cur.execute(delete_query)
    print(f'üóëÔ∏è  Deleted old cache')
    
    # –ë–∞—Ç—á-–≤—Å—Ç–∞–≤–∫–∞ —á–µ—Ä–µ–∑ –æ–¥–∏–Ω –±–æ–ª—å—à–æ–π INSERT (–≤ 100 —Ä–∞–∑ –±—ã—Å—Ç—Ä–µ–µ!)
    if stats:
        values_parts = []
        for stat in stats:
            campaign_name = stat.get('campaign_name', '').replace("'", "''")
            url = stat['url'].replace("'", "''")
            ctr = stat.get('ctr', 0) or 'NULL'
            cpc = stat.get('cpc', 0) or 'NULL'
            cpa = stat.get('cpa', 0) or 'NULL'
            
            values_parts.append(f"""(
                {project_id},
                '{stat['campaign_id']}',
                '{campaign_name}',
                '{url}',
                '{date_from}',
                '{date_to}',
                '{goal_id}',
                {stat.get('impressions', 0)},
                {stat.get('clicks', 0)},
                {stat.get('cost', 0)},
                {stat.get('conversions', 0)},
                {ctr},
                {cpc},
                {cpa},
                false,
                CURRENT_TIMESTAMP
            )""")
        
        insert_query = f"""
            INSERT INTO t_p97630513_yandex_cleaning_serv.rsya_platform_stats
            (project_id, campaign_id, campaign_name, url, date_from, date_to, goal_id,
             impressions, clicks, cost, conversions, ctr, cpc, cpa, is_blocked, cached_at)
            VALUES {','.join(values_parts)}
        """
        
        cur.execute(insert_query)
        print(f'‚úÖ Inserted {len(stats)} platforms')
    
    conn.commit()
    cur.close()
    print(f'üíæ Cache saved!')

def fetch_stats_from_api(token: str, campaign_ids: list, date_from: str, date_to: str, goal_id: str):
    url = 'https://api.direct.yandex.com/json/v5/reports'
    
    print(f'üöÄ Starting API request for {len(campaign_ids)} campaigns')
    print(f'üìÖ Date range: {date_from} to {date_to}')
    print(f'üéØ Goal ID: {goal_id or "None"}')
    
    field_names = ['CampaignId', 'CampaignName', 'Placement', 'Impressions', 'Clicks', 'Cost', 'Ctr', 'AvgCpc']
    
    # –í—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—è –∫–æ–Ω–≤–µ—Ä—Å–∏–π
    field_names.extend(['Conversions', 'CostPerConversion'])
    
    selection_criteria = {
        'DateFrom': date_from,
        'DateTo': date_to,
        'Filter': [
            {
                'Field': 'CampaignId',
                'Operator': 'IN',
                'Values': [str(cid) for cid in campaign_ids]
            },
            {
                'Field': 'Impressions',
                'Operator': 'GREATER_THAN',
                'Values': ['0']
            }
        ]
    }
    
    goals_params = None
    if goal_id and goal_id != '':
        goal_ids = [int(gid.strip()) for gid in goal_id.split(',') if gid.strip()]
        if goal_ids:
            # –Ø–Ω–¥–µ–∫—Å API –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –º–∞–∫—Å–∏–º—É–º 10 —Ü–µ–ª–µ–π
            if len(goal_ids) > 10:
                print(f'‚ö†Ô∏è  Too many goals ({len(goal_ids)}), using first 10')
                goals_params = goal_ids[:10]
            else:
                goals_params = goal_ids
    
    payload = {
        'params': {
            'SelectionCriteria': selection_criteria,
            'FieldNames': field_names,
            'ReportName': f'Platform Stats {datetime.now().isoformat()}',
            'ReportType': 'CUSTOM_REPORT',
            'DateRangeType': 'CUSTOM_DATE',
            'Format': 'TSV',
            'IncludeVAT': 'NO',
            'IncludeDiscount': 'NO'
        }
    }
    
    if goals_params:
        payload['params']['Goals'] = goals_params
    
    print(f'üìã Request payload: {json.dumps(payload, ensure_ascii=False)[:500]}')
    
    headers = {
        'Authorization': f'Bearer {token}',
        'Accept-Language': 'ru',
        'Client-Login': '',
        'skipReportHeader': 'true',
        'skipReportSummary': 'true'
    }
    
    import time
    
    print(f'‚è≥ Sending initial request to Yandex API...')
    response = requests.post(url, json=payload, headers=headers, timeout=120)
    
    print(f'üì° Initial response status: {response.status_code}')
    
    if response.status_code == 201 or response.status_code == 202:
        print(f'‚è∞ Report is being generated, waiting...')
        # –ñ–¥—ë–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –æ—Ç—á—ë—Ç–∞ (–º–∞–∫—Å–∏–º—É–º 120 –ø–æ–ø—ã—Ç–æ–∫ –ø–æ 3 —Å–µ–∫—É–Ω–¥—ã = 6 –º–∏–Ω—É—Ç)
        for attempt in range(120):
            print(f'üîÑ Attempt {attempt + 1}/120')
            time.sleep(3)
            
            retry_response = requests.post(url, json=payload, headers=headers, timeout=120)
            
            print(f'üì° Retry response status: {retry_response.status_code}')
            
            if retry_response.status_code == 200:
                print(f'‚úÖ Report ready! Parsing {len(retry_response.text)} bytes')
                parsed = parse_tsv_report(retry_response.text, goal_id)
                print(f'‚úÖ Parsed {len(parsed)} platforms')
                return parsed
            elif retry_response.status_code not in [201, 202]:
                print(f'‚ùå Report failed: {retry_response.text[:500]}')
                raise Exception(f'Report failed: {retry_response.status_code} - {retry_response.text[:200]}')
        
        print(f'‚è∞ Report generation timeout after 6 minutes')
        raise Exception('Report generation timeout (>6 minutes)')
    
    elif response.status_code == 200:
        print(f'‚úÖ Report ready immediately! Parsing {len(response.text)} bytes')
        parsed = parse_tsv_report(response.text, goal_id)
        print(f'‚úÖ Parsed {len(parsed)} platforms')
        return parsed
    else:
        print(f'‚ùå API error: {response.text[:500]}')
        raise Exception(f'API error: {response.status_code} - {response.text[:200]}')

def parse_tsv_report(tsv_text: str, goal_id: str):
    lines = tsv_text.strip().split('\n')
    
    if len(lines) < 2:
        return []
    
    header = lines[0].split('\t')
    print(f'üìã TSV Header: {header}')
    print(f'üìã First 3 data lines:')
    for i, line in enumerate(lines[1:4]):
        print(f'  Line {i+1}: {line}')
    
    platforms = []
    
    for line in lines[1:]:
        fields = line.split('\t')
        
        if len(fields) < 8:
            continue
        
        campaign_id = fields[0]
        campaign_name = fields[1]
        url = fields[2]
        impressions = int(fields[3]) if fields[3] else 0
        clicks = int(fields[4]) if fields[4] else 0
        cost_micro = float(fields[5]) if fields[5] else 0.0
        ctr = float(fields[6]) if fields[6] and fields[6] != '--' else 0.0
        cpc_micro = float(fields[7]) if fields[7] and fields[7] != '--' else 0.0
        
        # –Ø–Ω–¥–µ–∫—Å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç cost –∏ cpc –≤ –º–∏–∫—Ä–æ-–µ–¥–∏–Ω–∏—Ü–∞—Ö (1 —Ä—É–±–ª—å = 1 000 000)
        cost = cost_micro / 1_000_000
        cpc = cpc_micro / 1_000_000
        
        conversions = 0
        cpa_value = 0.0
        
        # –ü–∞—Ä—Å–∏–º –∫–æ–Ω–≤–µ—Ä—Å–∏–∏ (–≤—Å–µ–≥–¥–∞ –≤ 9-–º –ø–æ–ª–µ, –∏–Ω–¥–µ–∫—Å 8)
        if len(fields) > 8:
            conversions = int(fields[8]) if fields[8] and fields[8] != '--' else 0
        
        # –ü–∞—Ä—Å–∏–º CPA (–≤—Å–µ–≥–¥–∞ –≤ 10-–º –ø–æ–ª–µ, –∏–Ω–¥–µ–∫—Å 9)
        if len(fields) > 9:
            cpa_micro = float(fields[9]) if fields[9] and fields[9] != '--' else 0.0
            cpa_value = cpa_micro / 1_000_000
        elif conversions > 0:
            # –ï—Å–ª–∏ CPA –Ω–µ –ø—Ä–∏—à–µ–ª, —Å—á–∏—Ç–∞–µ–º –≤—Ä—É—á–Ω—É—é
            cpa_value = cost / conversions
        
        platform_dict = {
            'campaign_id': campaign_id,
            'campaign_name': campaign_name,
            'url': url,
            'impressions': impressions,
            'clicks': clicks,
            'cost': cost,
            'conversions': conversions,
            'ctr': ctr,
            'cpc': cpc,
            'cpa': cpa_value
        }
        platforms.append(platform_dict)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 3 –ø–ª–æ—â–∞–¥–∫–∏
        if len(platforms) <= 3:
            print(f'üìä Platform {len(platforms)}: {platform_dict}')
    
    return platforms